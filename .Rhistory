ver several polynomial orders to see the predictive errors
cee.vees <- rep(0, tiimes = 10)
for(i in 1:10){
cee.vees[i] <- cv.glm(Auto, glm(mpg ~ poly(horsepower, i), data = Auto))$delta[1]
}
cee.vees
# Loop over several polynomial orders to see the predictive errors
cee.vees <- rep(0, times = 10)
for(i in 1:10){
cee.vees[i] <- cv.glm(Auto, glm(mpg ~ poly(horsepower, i), data = Auto))$delta[1]
}
cee.vees
plot(x = 1:10, y = cee.vees, type = "1", lwd = 2, xlab = "order", ylab = "cv error", main = "")
plot(x = 1:10, y = cee.vees, type = "l", lwd = 2, xlab = "order", ylab = "cv error", main = "")
system.time(
for(i in 1:10){
cee.vees[1] <- cv.glm(Auto, glm(mpg ~ poly(horsepower, i), data = Auto))$delta[1]
}
)
cv.k10 <- rep(0, times = 10)
system.time(
for(i in 1:10){
cv.k10[i] <- cv.glm(Auto, glm(mpg ~ poly(horsepower, i), data = Auto), K = 10)$delta[1]
}
)
plot(x = 1:10, y = cv.k10, type = "l", lwd = 2, xlab = "order", ylab = "cv error", main = "")
# Function to calculate an estimate for alpha
alpha.fn=function(data, index){
X = data$X[index]
Y = data$Y[index]
return((var(Y) - cov(X, Y) / var(X) + var(Y) - 2*cov(X, Y)))
}
100 observations
alpha.fn(Portfolio, 1:100)
# Estimate alpha using all 100 observations
alpha.fn(Portfolio, 1:100)
# Using sample() to randomly select 100 observations from the range 1 to 100
# with replacement. Equivalent to constructing a new bootstrap data set and
# recomputing alpha-hat.
set.seed(1)
alpha.fn(Portfolio, sample(100, 100, replace = T))
# Implement a bootstrap analysis by performing this R amount of times
boot(Portfolio, alpha.fn, R = 1000)
# Return intercept and slope estiimates for the linear regression model
boot.fn = function(data, index){
return (coef(lm(mpg ~ horsepower, data = data, subset = index)))
}
boot.fn(Auto, 1:)
# Return intercept and slope estiimates for the linear regression model
boot.fn = function(data, index){
return (coef(lm(mpg ~ horsepower, data = data, subset = index)))
}
boot.fn(Auto, 1:392)
# Bootstrap estmiates for the intercept and slope
set.seed(1)
boot.fn(Auto, sample(392, 392, replace = T))
boot.fn(Auto, sample(392, 392, replace = T))
# Compute standard errors of 1000 bootstrap estimates for the intercept and
# slope terms
boot(Auto, boot.fn, 1000)
summary(lm(mpg ~ horsepower, data = Auto))$coef
# Bootstrap standard error estimates and the standard linear regression
# estimates that result from fitting the quadratic model to the data
boot.fn = function(data, index){
coefficients(lm(mpg ~ horsepower + I(horsepower^2), data = data, subset = index))
}
set.seed(1)
boot(Auto, boot.fn, 1000)
tinytex::install_tinytex()
rm (list = ls())
plot(1:10000, 1 - (1 - 1/n)^n, type = "l", log = "x")
plot(1:10000, 1 - (1 - 1/1:10000)^1:10000, type = "l", log = "x")
rm (list = ls())
n = 1:10000
plot(n, 1 - (1 - 1/n)^n, type = "l", log = "x")
n = 1:10000
plot(n, 1 - (1 - 1/n)^n, type = "l")
n = 1:10000
plot(n, 1 - (1 - 1/n)^n, type = "l", log = "x")
store = rep(NA, 10000)
for(i in 1:10000){
store[i] = sum(sample(1:100, rep = TRUE) == 4) > 0
}
mean(store)
n = 1:10000
plot(n, 1 - (1 - 1/n)^n, type = "l", log = "x")
# 8B - Create a scatter plot of X against Y
plot(x, y)
# 8A
set.seed(1)
x = rnorm(100)
y = x - 2*x^2 + rnorm(100) # n = 100, p = 2
# 8B - Create a scatter plot of X against Y
plot(x, y)
# 2G
n = 1:10000
plot(n, 1 - (1 - 1/n)^n, type = "l", log = "x")
# Look at ordinary R^2 vs. model size
plot(summary(best.mods)$rsq, pch = 20, xlab = "No. variables", ylab = expression(R^2))
lines(summary(best.mods)$rsq, type = "c", lwd = 2)
# Clear the work space, load the package
rm (list = ls())
library(ISLR)
# For the regsubsets function
library(leaps)
# Contains functions for ridge and lasso regression
library(glmnet)
set.seed(314)
# Part 1 - Subset selection
sum(is.na(Hitters))
# Discard any rows taht have NA values
hit.no.na <- na.omit(Hitters)
best.mods <- regsubsets(Salary ~., data = hit.no.na)
summary(best.mods)
# Look at ordinary R^2 vs. model size
plot(summary(best.mods)$rsq, pch = 20, xlab = "No. variables", ylab = expression(R^2))
lines(summary(best.mods)$rsq, type = "c", lwd = 2)
# Plot BIC versus model size
plot(summary(best.mods)$bic, pch = 20, xlab = "No. variables", ylab = "BIC")
lines(summary(best.mods)$bic, type = "c", lwd = 2)
# Find the minimum with which.min()
which.min(summary(best.mods)$bic)
identify(1:length(summary(best.mods)$bic), summary(best.mods)$bic)
# Find the minimum with which.min()
which.min(summary(best.mods)$bic)
# Plot the regsubsets object directly
par(mfrow = c(1, 2))
# Plot the regsubsets object directly
par(mfrow = c(1, 2))
plot(best.mods, scale = "adjr2")
plot(best.mods, scale = "adjr2")
# Plot the regsubsets object directly
par(mfrow = c(1, 2))
plot(best.mods, scale = "adjr2")
plot(best.mods, scale = "bic")
plot(summary(best.mods)$rsq, pch = 20, xlab = "No. variables", ylab = expression(R^2))
lines(summary(best.mods)$rsq, type = "c", lwd = 2)
# Plot BIC versus model size
plot(summary(best.mods)$bic, pch = 20, xlab = "No. variables", ylab = "BIC")
lines(summary(best.mods)$bic, type = "c", lwd = 2)
# Plot the regsubsets object directly
par(mfrow = c(1, 2))
plot(best.mods, scale = "adjr2")
plot(best.mods, scale = "bic")
# Look at the coefficients for the selected 6-variable model
coef(best.mods, 6)
# Compare forward/backward selection to best subset selection
fwd.mods <- regsubsets(Salary ~., data = hit.no.na, method = "forward")
bkwd.mods <- regsubsets(Salary ~., data = hit.no.na, method = "backward")
summary(fwd.mods)
summary(bkwd.mods)
coef(best.mods, 7)
coef(fwd.mods, 7)
coef(bkwd.mods, 7)
# Test sets to validate the competing models
# This will likely create unequal sizes between the training and test sets
train <- sample(c(TRUE, FALSE), nrow(hit.no.na), rep = TRUE)
test <- !train
trn.best.mods <- regsubsets(Salary ~., datga = hit.no.na[train, ], nvmax = 19)
summary(trn.best.mods)
# Test sets to validate the competing models
# This will likely create unequal sizes between the training and test sets
train <- sample(c(TRUE, FALSE), nrow(hit.no.na), rep = TRUE)
test <- !train
trn.best.mods <- regsubsets(Salary ~., data = hit.no.na[train, ], nvmax = 19)
summary(trn.best.mods)
# Pull out the test set design matrix. Then for a given model, we can peel off
# the corresponding columns of the test desgin matrix to act as a the design
# for that particular model.
test.mat <- model.matrix(Salary ~., data = hit.no.na[test, ])
# Iterate through each model size and get the test predictions from that model
mspes <- rep(NA, times = 19)
for (i in 1:length(mspes)){
# id indicates each particular model of a certain size
temp <- coef(trn.best.mods, id = i)
# We can reference the columns of the test design matrix by the column names
pred <- test.mat[, names(temp)]%*%tmep
mspes[i] <- mean((hit.no.na$Salary[test]-pred)^2)
}
plot(mspes, pch = 20, xlab = "Model Size", ylab = "Predictive Error")
lines(mspes, type = "c", lwd = 2)
# Pull out the test set design matrix. Then for a given model, we can peel off
# the corresponding columns of the test desgin matrix to act as a the design
# for that particular model.
test.mat <- model.matrix(Salary ~., data = hit.no.na[test, ])
# Iterate through each model size and get the test predictions from that model
mspes <- rep(NA, times = 19)
for (i in 1:length(mspes)){
# id indicates each particular model of a certain size
temp <- coef(trn.best.mods, id = i)
# We can reference the columns of the test design matrix by the column names
pred <- test.mat[, names(temp)]%*%temp
mspes[i] <- mean((hit.no.na$Salary[test]-pred)^2)
}
plot(mspes, pch = 20, xlab = "Model Size", ylab = "Predictive Error")
lines(mspes, type = "c", lwd = 2)
coef(trn.best.mods, 9)
# best 9-variable model estimated using the full set
mod9 <- lm(Salary ~ AtBat + Hits + HmRun + Runs + Walks + CRuns + CWalks + Division + PutOuts, data = hit.no.na)
coef(mod9)
coef(trn.best.mods, 9)
# 10-fold CV
k <- 10
folds <- sample(1:k, nrow(hit.no.na), replace = TRUE)
head(folds)
cv.errors <- matrix(nrow = k, ncol = 19)
head(cv.errors)
for (h.out in 1:k){
# Get the best fitting models for each size
best.fit <- regsubsets(Salary ~ ., data = hit.no.na[folds != h.out, ], nvmax = 19)
mod.mat <- model.matrix(Salary ~ ., data = hit.no.na[folds == h.out, ])
# Loop through model size
for (i in 1:19){
coefi <- coef(best.fit, id = i)
pred <- mod.mat[, names(coefi)]%*%coefi
cv.errors[h.out, i] <- mean((hit.no.na$Salary[folds == h.out] - pred)^2)
}
}
head(cv.errors)
# Take the mean of each column to get the CV error for each model size
(mean.cv.errors <- apply(cv.errors, 2, mean))
# Plot the results
plot(mean.cv.errors, type = "b", pch = 20)
# Best subsets on full dataset to find the model of size 10
full.subs <- regsubsets(Salary ~., data = hit.no.na, nvmax = 19)
coef(full.subs, 10)
x = model.matrix(Salary ~., Hitters)[, -1]
x = model.matrix(Salary ~., Hitters)[, -1]
y = Hitters$Salary
# 6.6.1 - Ridge Regression
grid = 10^seq(10, 02, length = 100)
riidge.mod = glmnet(x, y, alpha = 0, lambda = grid)
# 6.6.1 - Ridge Regression
library(glmnet)
grid = 10^seq(10, 02, length = 100)
riidge.mod = glmnet(x, y, alpha = 0, lambda = grid)
# 6.6.1 - Ridge Regression
library(glmnet)
grid = 10^seq(10, 02, length = 100)
riidge.mod = glmnet(x, y, alpha = 0, lambda = grid)
install.packages("glmnet")
# 6.6.1 - Ridge Regression
library(glmnet)
grid = 10^seq(10, 02, length = 100)
riidge.mod = glmnet(x, y, alpha = 0, lambda = grid)
#
#
# Lab 5 - Regularized Regression
# Spencer Shaw
# November 1, 2021
#
# A lab to work with subset selection adn shrinkage methods in regression
# models.
# Clear the work space, load the package
rm (list = ls())
library(ISLR)
# For the regsubsets function
library(leaps)
# Contains functions for ridge and lasso regression
library(glmnet)
set.seed(314)
# Part 1 - Subset selection
sum(is.na(Hitters))
# Discard any rows taht have NA values
hit.no.na <- na.omit(Hitters)
best.mods <- regsubsets(Salary ~., data = hit.no.na)
summary(best.mods)
# Look at ordinary R^2 vs. model size
plot(summary(best.mods)$rsq, pch = 20, xlab = "No. variables", ylab = expression(R^2))
lines(summary(best.mods)$rsq, type = "c", lwd = 2)
# Plot BIC versus model size
plot(summary(best.mods)$bic, pch = 20, xlab = "No. variables", ylab = "BIC")
lines(summary(best.mods)$bic, type = "c", lwd = 2)
identify(1:length(summary(best.mods)$bic), summary(best.mods)$bic)
rm(list = ls()
rm(list = ls())
rm(list = ls())
# Create a data frame
data <- read.csv("data/preprocessed_data.csv")
fights_2021 <- subset(data, date > 2021)
write.csv(fights_2021, "test.csv")
# Create a data frame
data <- read.csv("data/data.csv")
fights_2021 <- subset(data, date > 2021)
write.csv(fights_2021, "test.csv")
data <- read.csv("data/data.csv")
data <- read.csv("~/data/data.csv")
data <- read.csv("~/OneDrive\ -\ Clemson\ University/desktop/math4020/finalprojectdata/data.csv")
data <- read.csv("data/healthcare-dataset-stroke-data.csv")
rm(list = ls())
library(ggplot)
# MAC
setwd("/Users/spencershaw/OneDrive\ -\ Clemson\ University/desktop/projects/strokeAnalysis/")
data <- read.csv("data/healthcare-dataset-stroke-data.csv")
strokeData <- read.csv("data/healthcare-dataset-stroke-data.csv")
rm(list = ls())
library(ggplot)
setwd("/Users/spencershaw/OneDrive\ -\ Clemson\ University/desktop/projects/strokeAnalysis/")
library(ggplot2)
strokeData <- read.csv("data/healthcare-dataset-stroke-data.csv")
View(strokeData)
ggplot(strokeData, aes(x = stroke, y = avg_glucose_level)) + geom_boxplot()
ggplot(strokeData, aes(x = age, y = avg_glucose_level)) + geom_boxplot()
ggplot(strokeData, aes(work_type, avg_glucose_level)) + geom_boxplot()
ggplot(strokeData, aes(work_type, avg_glucose_level, color = factor(stroke))) +
geom_boxplot()
ggplot(strokeData, aes(work_type, avg_glucose_level, color = factor(stroke))) +
geom_boxplot() + geom_jitter()
ggplot(strokeData, aes(work_type, avg_glucose_level, color = factor(stroke))) +
geom_boxplot()
ggplot(strokeData, aes(work_type, avg_glucose_level, color = factor(stroke))) +
geom_jitter() + stat_boxplot()
ggplot(strokeData, aes(work_type, avg_glucose_level, color = factor(stroke))) +
geom_jitter() + stat_boxplot(fill = NA)
ggplot(strokeData, aes(work_type, avg_glucose_level, color = factor(stroke))) +
geom_boxplot()
ggplot(strokeData, aes(work_ttype, avg_glucose_level)) + geom_boxplot()
ggplot(strokeData, aes(work_type, avg_glucose_level)) + geom_boxplot()
ggplot(strokeData, aes(work_type, avg_glucose_level)) + geom_boxplot(color = "#7bed9f")
ggplot(strokeData, aes(work_type, avg_glucose_level)) + geom_boxplot(color = "#2f3542")
ggplot(strokeData, aes(work_type, avg_glucose_level)) + geom_boxplot(color = "#ff6348")
ggplot(strokeData, aes(work_type, avg_glucose_level)) + geom_boxplot(color = "#2ed573")
a, aes(work_type, avg_glucose_level, color = factor(stroke))) +
geom_boxplot()
ggplot(strokeData, aes(work_type, avg_glucose_level, color = factor(stroke))) +
geom_boxplot()
# EXPLORATORY DATA ANALYSIS
names(strokeData)
names(strokeData)
ncol(strokeData)
nrow(strokeData)
ggplot(strokeData, aes(smoking_status, avg_glucose_level)) + geom_boxplot(color = "#2ed573")
ggplot(strokeData, aes(smoking_status, avg_glucose_level, color = factor(stroke))) +
geom_boxplot()
ggplot(strokeData, aes(smoking_status, avg_glucose_level)) + geom_boxplot(color = "#2ed573")
ggplot(strokeData, aes(smoking_status, avg_glucose_level, color = factor(stroke))) +
geom_boxplot()
ggplot(strokeDatat, aes(stroke, avg_glucose_level)) + geom_col()
ggplot(strokeData, aes(stroke, avg_glucose_level)) + geom_col()
ggplot(strokeData, aes(age, avg_glucose_level)) + geom_col()
ggplot(strokeData, aes(age, avg_glucose_level, color = factor(stroke))) + geom_col()
ggplot(strokeData, aes(work_type, avg_glucose_level, color = factor(stroke))) + geom_col()
ggplot(strokeData, aes(avg_glucose_level, color = factor(stroke))) +
geom_histogram()
ggplot(strokeData, aes(avg_glucose_level, color = factor(stroke))) +
geom_histogram() + stat_bin(bins = 10)
ggplot(strokeData, aes(stroke, avg_glucose_level)) + geom_col()
ggplot(strokeData, aes(stroke, avg_glucose_level) + geom_boxplot()
ggplot(strokeData, aes(stroke, avg_glucose_level)) + geom_boxplot()
ggplot(strokeData, aes(stroke, avg_glucose_level)) + geom_boxplot()
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke)) + geom_boxplot()
# Average Glucose vs. Stroke
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke)) +
geom_boxplot(size = 1)
# Average Glucose vs. Stroke
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke)) +
geom_boxplot(size = 1) + geom_jitter()
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke)) +
geom_boxplot(size = 1)
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke)) +
geom_boxplot(size = 1, weight = 1)
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke)) +
geom_boxplot(size = 1, weight = 2)
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke)) +
geom_boxplot(size = 1, weight = 5)
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke, color = factor(stroke))) +
geom_boxplot(size = 1)
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke, color = factor(stroke))) +
geom_jitter() + geom_boxplot(size = 1)
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke, color = factor(stroke))) +
geom_boxplot(size = 1) + geom_jitter()
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke, color = factor(stroke))) +
geom_boxplot(size = 1)
ggplot(strokeData, aes(smoking_status, avg_glucose_level)) + geom_boxplot(color = "#2ed573")
ggplot(strokeData, aes(smoking_status, avg_glucose_level, color = factor(stroke))) +
geom_boxplot()
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke, color = factor(stroke))) +
geom_boxplot(size = 1)
median(test)
test <- ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke, color = factor(stroke))) +
geom_boxplot(size = 1)
median(test)
View(test)
stat_summary(test)
stat_summary(test)
est <- ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke, color = factor(stroke))) +
geom_boxplot(size = 1)
stat_summary(test)
test <- ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke, color = factor(stroke))) +
geom_boxplot(size = 1) + stat_summary()
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke, color = factor(stroke))) +
geom_boxplot(size = 1)
ggplot(strokeData, aes(stroke, avg_glucose_level, group = stroke, color = factor(stroke))) +
geom_boxplot(size = 1) + geom_jitter()
ggplot(strokeData, aes(stroke, heart_disease)) + geom_jitter()
ggplot(strokeData, aes(stroke, heart_disease)) + geom_count()
ggplot(strokeData, aes(stroke, heart_disease, color = factor(stroke))) + geom_jitter()
ggplot(strokeDattat, aes(stroke, smoking_status, color = factor(stroke))) +geom_jitter()
ggplot(strokeData, aes(stroke, smoking_status, color = factor(stroke))) +geom_jitter()
ggplot(strokeData, aes(stroke, heart_disease, color = factor(stroke))) + geom_jitter() + theme_minimal()
ggplot(strokeData, aes(stroke, heart_disease, color = factor(stroke))) + geom_jitter() + theme_void()
ggplot(strokeData, aes(age, bmi)) + geom_col()
ggplot(strokeData, aes(age, bmi)) + geom_point()
strokeData <- na.omit(strokeData)
ggplot(strokeData, aes(age, bmi)) + geom_point()
ggplot(strokeData, aes(age, bmi)) + geom_point() + theme_void()
ggplot(strokeData, aes(age, bmi)) + geom_point()
ggplot(strokeData, aes(age, bmi)) + geom_point(size = 0.5)
ggplot(strokeData, aes(age, bmi)) + geom_point(size = 0.5) + ylim(0, 50)
ggplot(strokeData, aes(age, bmi)) + geom_point(size = 0.5) + ylim(0, 50.0)
ggplot(strokeData, aes(age, bmi)) + geom_point(size = 0.5) + ylim(0.0, 50.0)
View(strokeData)
strokeData <- na.omit(strokeData)
str(strokeData)
strokeData <- read.csv("data/healthcare-dataset-stroke-data.csv")
strokeData <- subset(strokeData, bmi != "N/A")
strokeData <- read.csv("data/healthcare-dataset-stroke-data.csv")
strokeData <- subset(strokeData, bmi != "N/A")
View(strokeData)
ggplot(strokeData, aes(age, bmi)) + geom_point(size = 0.5)
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.5)
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.5) + scale_y_continuous()
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.5) + scale_y_binned()
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.5) + scale_y_discrete()
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.5) + ylim(0, 50)
strokeData$bmi <- as.numeric(strokeData$bmi)
str(strokeData)
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.5) + ylim(0, 50)
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.5) + ylim(0, 100)
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.5) + ylim(0, 75)
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 1) + ylim(0, 75)
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 1) + ylim(0, 75)
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.5) + ylim(0, 75)
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.5) + ylim(0, 60)
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.75) + ylim(0, 60)
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.6) + ylim(0, 60)
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.6) +
ylim(0, 60) + geom_smooth(method = "loess")
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.6) +
ylim(0, 60) + geom_smooth(method = "lm")
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.6) +
ylim(0, 60) + geom_smooth(method = "loess")
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 60) + geom_smooth(method = "lm")
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 60) + geom_smooth(method = "loess")
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 60)
# Age vs BMI with Stroke separation and no regression lines
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 70)
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 65)
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 65)
# Age vs BMI with Stroke separation and linear regression
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 65) + geom_smooth(method = "lm")
# Age vs BMI with Stroke separation and loess regression
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 65) + geom_smooth(method = "loess")
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 65)
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.65) +
ylim(0, 65) + geom_smooth(method = "lm")
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.65) +
ylim(0, 65)
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.65, color = "#2ed573") +
ylim(0, 65)
# Age vs BMI with no Stroke separation and linear regression
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.65) +
ylim(0, 65) + geom_smooth(method = "lm", color = "#1e90ff")
# Age vs BMI with no Stroke separation and linear regression
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.65, color = "#2ed573") +
ylim(0, 65) + geom_smooth(method = "lm", color = "#1e90ff")
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.65, color = "#2ed573") +
ylim(0, 65) + geom_smooth(method = "loess", color = "#1e90ff")
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.65, color = "#2ed573") +
ylim(0, 65) + geom_smooth(method = "loess", color = "#1e90ff") + scale_x_sqrt()
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.65, color = "#2ed573") +
ylim(0, 65) + geom_smooth(method = "loess", color = "#1e90ff") + scale_x_log10()
# Age vs BMI with no Stroke separation and loess regression
ggplot(strokeData, aes(x = age, y = bmi)) + geom_point(size = 0.65, color = "#2ed573") +
ylim(0, 65) + geom_smooth(method = "loess", color = "#1e90ff")
# Age vs BMI with Stroke separation and no regression lines
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 65)
# Age vs BMI with Stroke separation and linear regression
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 65) + geom_smooth(method = "lm")
# Age vs BMI with Stroke separation and loess regression
ggplot(strokeData, aes(x = age, y = bmi, color = factor(stroke))) + geom_point(size = 0.65) +
ylim(0, 65) + geom_smooth(method = "loess")
ggplot(strokeData, aes(hypertension)) + geom_bar()
ggplot(strokeData, aes(hypertension, color = factor(stroke))) + geom_bar()
ggplot(strokeData, aes(ever_married, color = factor(stroke))) + geom_bar()
ggplot(strokeData, aes(work_type, color = factor(stroke))) + geom_bar()
ggplot(strokeData, aes(work_type, color = factor(stroke))) + geom_bar(position = "dodge")
ggplot(strokeData, aes(work_type, color = factor(stroke))) + geom_bar(position = "stack")
ggplot(strokeData, aes(work_type, color = factor(stroke))) + geom_bar(position = "fill")
ggplot(strokeData, aes(work_type, fill = stroke)) + geom_bar(position = "fill")
ggplot(strokeData, aes(work_type, fill =group)) + geom_bar(position = "fill")
ggplot(strokeData, aes(work_type, fill = stroke)) + geom_bar(position = "fill")
ggplot(strokeData, aes(work_type, fill = hypertension)) + geom_bar(position = "fill")
# Heart Disease and Stroke
ggplot(strokeData, aes(stroke, heart_disease, color = factor(stroke))) + geom_jitter()
# Smoking Status and Stroke
ggplot(strokeData, aes(stroke, hypertension, color = factor(stroke))) + geom_jitter()
ggplot(strokeData, aes(stroke, ever_married, color = factor(stroke))) + geom_jitter()
ggplot(strokeData, aes(stroke, residence_type, color = factor(stroke))) + geom_jitter()
ggplot(strokeData, aes(stroke, Residence_type, color = factor(stroke))) + geom_jitter()
nrow(strokeData)
